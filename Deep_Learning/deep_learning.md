# Deep Learning
# Implizite Merkmalsextraktion durch neuronale Netze
Einen Teilbereich der künstlicher Intelligenz stellt das maschinelle Lernen dar. Hier erfolgt eine Mustererkennung nicht nach vorher festgelegten Regeln, sondern anhand intrinsischer Merkmale der Datenpunkte. Diese können im Fall des unüberwachten Lernens mittels einer definierten Ähnlichkeitsmetrik, wie z.B. des geometrischen Abstands, gruppiert werden. Beim überwachten Lernen findet in einer vorhergehenden Datenanalyse eine Klassifizierung der in dieser Phase vorkommenden Merkmale statt. Anschließend werden diese Klassen separiert und die daraus abgeleiteten Klassengrenzen für die Klassifizierung von Daten außerhalb der Trainingsphase verwendet.
Zur sinnvollen Trennung der Daten muss oft im Vorfeld eine Merkmalsextraktion und -selektion stattfinden, dies bedeutet, genau die Merkmale zu finden, die den Datensatz noch gut genug beschreiben und sinnvoll trennen. Merkmale, die eine geringe Repräsentation besitzen werden verworfen. Eine Anwendung des klassischen maschinellen Lernens geht dadurch bei multivarianten Daten oft mit einer manuellen Vorarbeit einher. Auf diese manuelle Merkmalsextraktion kann bei der Anwendung des tiefen Lernens verzichtet werden. Die beim tiefen Lernen angewandten Topologien neuronaler Netze ermöglichen eine implizite Merkmalsextraktion durch automatische Anpassung ihrer Parameter aufgrund von Trainingsdaten.
  * Mittels tiefer neuronaler Netze können Merkmale ohne vorherige Featureextraktion herausgebildet werden
  
## Neuronale Netze
  
### McCulloch/Pitts Zelle
Der Beginn der Entwicklung künstlicher neuronaler Netze wird oft auf das 1943 von Warren McCulloch und Walter Pitts vorgestellte „künstliche Neuron“ datiert. Inspiriert von der damaligen Grundlagenforschung zu biologischen Neuronen stellt das „künstliche Neuron“ ein einfaches Modell dar, das die Signalweiterleitung im Gehirn simulieren soll. Dieses Neuron besteht aus mehreren Eingangsleitungen und einer Ausgangsleitung, die jeweils binäre Signale verarbeiten oder ausgeben können. Die Eingangsleitungen können dabei das Attribut „hemmend“ oder „bestärkend“ besitzen. Ist mindestens eine hemmende Eingangsleitung aktiv, wird der Ausgang des Neurons inaktiv geschaltet. Sind eine vorgegebene Menge an bestärkenden Eingangsleitungen aktiv, schaltet das Neuron den Ausgang aktiv. Durch Vernetzung mehrerer derartiger künstlicher Neuronen ist die Berechnung komplexer logischer Verknüpfungen möglich. Die Netze werden dann für die jeweilige Aufgabe im Vornherein parametriert, indem die hemmenden und bestärkenden Leitungen und die Schaltschwellen vorgegeben werden

![](Images/mcculloch.png "McCulloch Pitts")
  
### Multilayer Perzeptron
Im Jahre 1957 stellte Frank Rosenblatt das sogenannte Perzeptron als ein erweitertes Modell eines künstlichen Neurons vor. Die Signale sind hier nicht mehr binär, sondern können Zahlenwerte annehmen. Jede Eingangsleitung besitzt eine Gewichtung mit der die einzelnen Signalwerte multipliziert werden. Die Summe dieser gewichteten Werte wird als Eingabewert einer sogenannten Aktivierungsfunktion verwendet. Der Ausgabewert dieser Funktion ist der des Perzeptrons. Als Aktivierungsfunktion wird zum Beispiel die Heaviside-Funktion verwendet.
  
ℎ𝑒𝑎𝑣𝑖𝑠𝑖𝑑𝑒(𝑧)={
    0 𝑤𝑒𝑛𝑛 𝑧<0 
    1 𝑤𝑒𝑛𝑛 𝑧≥0
}
  
Durch Hintereinanderschaltung mehrerer paralleler Perzeptronen wurde die Basis moderner künstlicher neuronaler Netze gelegt. Die Topologie derartiger Netze besteht aus einer Eingabeschicht, die jedes Eingangssignal an jedes in der dahinterliegenden Schicht liegende Neuron weiterleitet. Auf diese Schicht können dann beliebig viele weitere Schichten gestapelt werden. Die letzte dieser Schichten wird als Ausgabeschicht bezeichnet, die zwischen dieser und der Eingabeschicht liegenden als verborgende Schichten. Liegt ein Netz mit insgesamt drei Schichten vor, wird dieses Netz als Multilayer Perzeptron bezeichnet. Erst 1986 wurde mit dem Backpropagation-Algorithmus und der Ersetzung der Heaviside-Aktivierungsfunktion mit einer stetig differenzierbaren Funktion die Möglichkeit gegeben neuronale Netze aufgrund von Daten zu trainieren
  
### Tiefe neuronale Netze
Mit der Verwendung mehrerer verborgener Schichten nimmt die Abstraktionsfähigkeit des neuronalen Netzes zu. Dies liegt an der mit der Schichttiefe exponentiell steigenden Anzahl von Parametern, u.a. der Kantengewichte. Mittels dieser tiefen Schichten, bei denen sich die Anzahl der Neuronen pro Schicht für gewöhnlich verringern, sind komplexe Mustererkennungen, wie z.B. von Gesichtern möglich. Allerdings geht dieser Anstieg der Leistungsfähigkeit mit Problemen einher. Zum einen tritt beim Training mittels Backpropagation-Verfahren das Problem des explodierenden oder verschwindenden Gradienten auf. Wird ein Fehler von der Ausgabeschicht zur Eingabeschicht zurückpropagiert, wird dieser immer kleiner. Im schlechtesten Fall werden die untersten Schichten gar nicht trainiert. Weiter besteht das Problem der Überanpassung. Je mehr Parameter das neuronale Netz besitzt, desto eher neigt es dazu, den Trainingsdatensatz als solches zu repräsentieren und nicht die Muster der Daten des Trainingssatzes zu extrahieren. Um einer Überanpassung zu entgehen, muss die Menge an Trainingsdaten größer werden. Damit steigt dann auch die Dauer des Trainings
  
### Rekurrente neuronale Netze
Bei den bislang vorgestellten Netztopologien handelt es sich um Feed-Forward-Netze. Ein Eingabevektor wird hier durch das Netz bis zur Ausgabeschicht propagiert. Dies ist für statische Daten wie Bilder sinnvoll, es ermöglicht allerdings keine Einprägung zeitlicher Muster. Bei rekurrenten Netzen wird diese Abhängigkeit zeitlicher Muster durch die Einführung von rückwärts gerichteten Verbindungen innerhalb des Netzes erreicht. Im einfachsten Fall besteht ein derartiges Netz aus einem Neuron, das zum nächsten Zeitpunkt t zusätzlich zum neuen Eingabewert seine Ausgabe vom letzten Zeitpunkt t-1 erhält. Werden mehrere derartige Neuronen in einer Schicht kombiniert, erhalten diese neben einem neuen Eingabevektor die Ausgabe des letzten Ausführungszeitpunktes. Nachteil dieses einfachen Netzes ist, dass zeitliche Eingaben zu frühen Zeitpunkten aufgrund der andauernden Transformation mittels neuer Daten langsam „verblassen“. Die Netztopologie besitzt damit kein Langzeitgedächtnis. Komplexere Zellen, wie die LSTM-Zelle oder GRU-Zelle verringern dieses Problem. Die Long-Short-Term Memory-Zelle (LSTM) besitzt so zwei Zustandsvektoren, von denen der eine die kurzfristigen Merkmale und der andere die längerfristigen Merkmale abspeichern soll